# DF

```python
%pyspark

# HDFS에서 로그 파일 읽기
file_path = 'hdfs://localhost:9000/input/logs/2024-01-01.log'
df = spark.read.csv(file_path, sep= ' ')  # 공백을 구분자로 CSV 파일 읽기
df.show()  # 데이터프레임 내용 출력
```

```python
%pyspark
# 데이터프레임의 컬럼 이름 확인
df.columns
```

```python
# 데이터프레임의 스키마 출력
df.printSchema()
```

```python
%pyspark
# 특정 컬럼 선택하여 출력
df.select('_c0', '_c1').show()
```

```python
%pyspark
# 데이터프레임에서 상위 2개 행 가져오기
df.take(2)
```

```python
%pyspark
# 데이터프레임을 Pandas 데이터프레임으로 변환 후 특정 컬럼 선택
pd_df = df.toPandas()
pd_df[['_c0', '_c2']]
```

```python
%pyspark
# 특정 컬럼 선택하여 출력
df.select(df._c2).show()
```

```python
%pyspark
from pyspark.sql.functions import split, col

# '_c2' 컬럼을 공백으로 분리하여 'method', 'path', 'protocol' 컬럼 생성
df = df.withColumn('method', split(col('_c2'), ' ').getItem(0))
df = df.withColumn('path', split(col('_c2'), ' ').getItem(1))
df = df.withColumn('protocol', split(col('_c2'), ' ').getItem(2))

# 'method'가 'POST'인 데이터 필터링
df.filter(df.method == 'POST').show()
```

```python
%pyspark
# 'method' 컬럼 기준으로 그룹화하여 개수 계산
df.groupby('method').count().show()
```

```python
%pyspark

# HDFS에서 로그 파일 읽기 및 임시 뷰 생성
file_path = 'hdfs://localhost:9000/input/logs/2024-01-01.log'
df = spark.read.csv(file_path, sep= ' ')
df.createOrReplaceTempView('logs')  # SQL 쿼리를 위한 임시 뷰 생성
```

```python
%pyspark
# SQL 쿼리를 사용하여 데이터 조회
spark.sql('''
    SELECT * FROM logs
    ''').show()
```

```python
%pyspark
# SQL 쿼리를 사용하여 '_c2' 컬럼을 분리하고 새로운 컬럼 추가
df = spark.sql('''
    select *, split(_c2, ' ')[0] as method, split(_c2, ' ')[1] as path, split(_c2, ' ')[2] as protocol
    from logs
''')
df.show()
```

```python
%pyspark
# 새로운 임시 뷰 생성
df.createOrReplaceTempView('logs2')
```

```python
%pyspark
# SQL 쿼리를 사용하여 '_c3' 값이 400인 데이터 필터링
spark.sql('''
    select * from logs2
    where _c3 = 400
''').show()
```

```python
%pyspark
# SQL 쿼리를 사용하여 '_c3' 값이 200이고 'path'에 'product'가 포함된 데이터 필터링
spark.sql('''
    select * from logs2
    where _c3 = 200 and path like '%product%'
''').show()
```

```python
%pyspark
# SQL 쿼리를 사용하여 'method'별 요청 수 계산
spark.sql('''
    select method, count(*) from logs2
    group by method
''').show()
```