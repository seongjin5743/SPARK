# STREAMING

```python
from pyspark.sql.functions import current_timestamp  # 현재 타임스탬프를 추가하기 위한 함수 임포트

# 소켓 스트림에서 데이터를 읽어옴 (localhost:9999)
lines = spark.readStream.format('socket') \
        .option('host', 'localhost') \
        .option('port', '9999') \
        .load()
        
# 데이터에 현재 타임스탬프 추가
lines_with_time = lines.select(
    lines.value,  # 스트림 데이터의 value 컬럼 선택
    current_timestamp()  # 현재 타임스탬프 추가
)

# 데이터를 HDFS에 CSV 형식으로 저장 (append 모드)
query = lines_with_time.writeStream \
        .outputMode('append') \
        .format('csv') \
        .option('path', 'hdfs://localhost:9000/output/stream-test') \
        .option('checkPointLocation', 'hdfs://localhost:9000/output/stream-temp') \
        .start()
        
# 스트림 종료 대기
query.awaitTermination()
```

## wordcount
```python
from pyspark.sql.functions import split, explode  # 문자열 분리 및 행 확장을 위한 함수 임포트

# 소켓 스트림에서 데이터를 읽어옴 (localhost:9999)
lines = spark.readStream.format('socket').option('host', 'localhost').option('port', '9999').load()

# 데이터를 공백으로 분리하고 각 단어를 개별 행으로 확장
words = lines.select(
    explode(split(lines['value'], ' '))  # 공백으로 분리 후 explode
)

# 각 단어의 빈도를 계산
word_count = words.groupBy('col').count()

# 결과를 메모리에 저장하고 10초마다 갱신
query = word_count.writeStream \
        .trigger(processingTime='10 seconds') \
        .outputMode('complete') \
        .format('memory')\
        .queryName('word')\
        .start()
```

```python
query.stop()  # 스트림 종료
```

```python
df = spark.table('word')  # 메모리에 저장된 테이블 읽기
df.show()  # 테이블 내용 출력
```

## spark - kafka
- Zeppelin 설정: Interpreter => spark
- spark.jars.packages => org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.4
```python
from pyspark.sql.functions import split, col  # 문자열 분리 및 컬럼 선택을 위한 함수 임포트

# Kafka 스트림에서 데이터를 읽어옴
df_stream_kafka = spark.readStream.format('kafka')\
                .option('kafka.bootstrap.servers', 'localhost:9092')\  # Kafka 브로커 주소
                .option('subscribe', 'temp')\  # 구독할 Kafka 토픽
                .load()

# Kafka 메시지 데이터를 파싱하여 컬럼으로 분리
df_parsed = df_stream_kafka.select(
    split(col('value'), ',')[0].alias('market'),  # market 컬럼
    split(col('value'), ',')[1].alias('trade_date'),  # 거래 날짜
    split(col('value'), ',')[2].alias('trade_time'),  # 거래 시간
    split(col('value'), ',')[3].alias('trade_price'),  # 거래 가격
)

# 데이터를 HDFS에 CSV 형식으로 저장 (10초마다 갱신)
query = df_parsed.writeStream\
        .trigger(processingTime='10 seconds')\
        .outputMode('append')\
        .format('csv')\
        .option('path', 'hdfs://localhost:9000/output/upbit-stream')\
        .option('checkpointLocation', 'hdfs://localhost:9000/output/upbit-temp')\
        .start()
```

```python
df = spark.read.csv('hdfs://localhost:9000/output/upbit-stream', inferSchema=True)  # HDFS에서 데이터 읽기
df.count()  # 데이터 개수 출력
```

```python
z.show(df)  # Zeppelin에서 데이터 시각화
```

```python
from pyspark.sql.functions import format_number  # 숫자 포맷팅을 위한 함수 임포트

result = df.select('_c3').groupBy().avg('_c3')  # '_c3' 컬럼의 평균 계산
result.select(format_number('avg(_c3)', 2)).show()  # 평균값을 소수점 2자리로 포맷팅하여 출력
```

## log data
```python
# Kafka 스트림에서 로그 데이터를 읽어옴
raw = spark.readStream\
        .format('kafka')\
        .option('kafka.bootstrap.servers', 'localhost:9092')\  # Kafka 브로커 주소
        .option('subscribe', 'logs')\  # 구독할 Kafka 토픽
        .option('startingOffsets', 'latest')\  # 최신 오프셋부터 시작
        .load()
        
# 로그 데이터를 파싱하여 컬럼으로 분리
df = raw.select(
    split(col('value'), ' ')[0].alias('ip'),  # IP 주소
    split(col('value'), ' ')[1].alias('timestamp'),  # 타임스탬프
    split(col('value'), ' ')[2].alias('method'),  # HTTP 메서드
    split(col('value'), ' ')[3].alias('path'),  # 요청 경로
    split(col('value'), ' ')[4].alias('protocol'),  # 프로토콜
    split(col('value'), ' ')[5].alias('status_code'),  # 상태 코드
    split(col('value'), ' ')[6].alias('size'),  # 응답 크기
)

# 데이터를 HDFS에 CSV 형식으로 저장 (10초마다 갱신)
query = df.writeStream\
        .trigger(processingTime='10 seconds')\
        .outputMode('append')\
        .format('csv')\
        .option('path', 'hdfs://localhost:9000/output/log-stream')\
        .option('checkpointLocation', 'hdfs://localhost:9000/output/log-temp')\
        .option('header', 'true')\
        .start()
```

```python
query.stop()  # 스트림 종료
```

```python
logs_df = spark.read.csv('hdfs://localhost:9000/output/log-stream', header=True)  # HDFS에서 로그 데이터 읽기
logs_df.show()  # 로그 데이터 출력
```

```python
logs_df.groupBy('status_code').count().show()  # 상태 코드별 로그 개수 출력
```