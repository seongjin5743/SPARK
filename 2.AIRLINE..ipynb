{
  "metadata": {
    "name": "2.AIRLINE",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfile_path \u003d \u0027hdfs://localhost:9000/input/airline\u0027\n\ndf \u003d spark.read.csv(file_path, header\u003dTrue, inferSchema\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(df)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import col\n\ndf \u003d df.select(\n    \u0027Month\u0027,\n    \u0027DayOfWeek\u0027,\n    \u0027DayofMonth\u0027,\n    \u0027Origin\u0027,\n    \u0027Dest\u0027,\n    \u0027Cancelled\u0027,\n    \u0027UniqueCarrier\u0027,\n    \u0027Distance\u0027,\n    \u0027FlightNum\u0027,\n    col(\u0027AirTime\u0027).cast(\u0027int\u0027),\n    col(\u0027ArrTime\u0027).cast(\u0027int\u0027),\n    col(\u0027ArrDelay\u0027).cast(\u0027int\u0027),\n    col(\u0027DepTime\u0027).cast(\u0027int\u0027),\n    col(\u0027DepDelay\u0027).cast(\u0027int\u0027),\n    col(\u0027ActualElapsedTime\u0027).cast(\u0027int\u0027),\n    col(\u0027CRSElapsedTime\u0027).cast(\u0027int\u0027),\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.createOrReplaceTempView(\u0027airline\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect * from airline limit 10\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect distinct UniqueCarrier from airline\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.select(\u0027UniqueCarrier\u0027).distinct().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import *\ndf.groupBy(\u0027UniqueCarrier\u0027).agg(count(\u0027*\u0027)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect dayofweek, avg(DepDelay), avg(arrdelay)\nfrom airline\ngroup by dayofweek\norder by dayofweek;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.groupBy(\u0027DayOfWeek\u0027).agg(avg(\u0027DepDelay\u0027), avg(\u0027ArrDelay\u0027)).orderBy(\u0027DayOfWeek\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect uniquecarrier, month, count(*), avg(depdelay)\nfrom airline\ngroup by uniquecarrier, month;"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.groupBy(\u0027UniqueCarrier\u0027, \u0027Month\u0027).agg(count(\u0027*\u0027), avg(\u0027DepDelay\u0027)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\ndf.groupBy(\u0027UniqueCarrier\u0027)\\\r\n    .agg(\r\n        sum(\u0027Cancelled\u0027).alias(\u0027flight_cancelled_count\u0027),\r\n        sum(when(df.Cancelled \u003d\u003d 0, 1).otherwise(0)),\r\n        count(\u0027*\u0027).alias(\u0027total_count\u0027),\r\n    ).withColumn(\u0027cancel_rate\u0027, col(\u0027flight_cancelled_count\u0027) / col(\u0027total_count\u0027)*100).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\r\nSELECT\r\n    *,\r\n    (flight_cancelled_count / total_count * 100) AS cancel_rate\r\nFROM\r\n(SELECT\r\n    UniqueCarrier,\r\n    SUM(Cancelled) AS flight_cancelled_count,\r\n    SUM(CASE WHEN Cancelled \u003d\u003d 0 THEN 1 ELSE 0 END),\r\n    COUNT(*) AS total_count\r\nFROM airline\r\nGROUP BY UniqueCarrier)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\norigin_df \u003d df.groupBy(\u0027Origin\u0027).count()\r\n\r\ndest_df \u003d df.groupBy(\u0027Dest\u0027).count()\r\n\r\norigin_df.join(dest_df, origin_df.Origin \u003d\u003d dest_df.Dest).withColumn(\u0027total\u0027, origin_df[\u0027count\u0027] + dest_df[\u0027count\u0027]).orderBy(desc(\u0027total\u0027)).show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT *, origin_count + dest_count AS total\nFROM\n(\n(SELECT Origin, COUNT(*) AS origin_count\nFROM airline\nGROUP BY Origin) AS origin_airline\n\nJOIN\n\n(SELECT Dest, COUNT(*) AS dest_count\nFROM airline\nGROUP BY Dest) AS dest_airline\n\nON origin_airline.Origin \u003d\u003d dest_airline.Dest\n)\nORDER BY total DESC LIMIT 10;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\ndf.groupBy(\u0027Origin\u0027, \u0027Dest\u0027) \\\r\n    .agg(\r\n        avg(\u0027ActualElapsedTime\u0027).alias(\u0027real_time\u0027),\r\n        avg(\u0027CRSElapsedTime\u0027).alias(\u0027crs_time\u0027)\r\n    ).withColumn(\u0027diff_time\u0027, abs(col(\u0027real_time\u0027)-col(\u0027crs_time\u0027))) \\\r\n    .orderBy(desc(\u0027diff_time\u0027)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT\n    *, ABS(real_time - crs_time) AS diff_time\nFROM\n(SELECT \n    Origin, \n    Dest, \n    AVG(ActualElapsedTime) AS real_time, \n    AVG(CRSElapsedTime) AS crs_time\nFROM airline\nGROUP BY Origin, Dest)\nORDER BY diff_time DESC"
    }
  ]
}